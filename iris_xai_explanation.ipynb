{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76895df4",
   "metadata": {},
   "source": [
    "# Explainable AI (XAI) Methods Implementation\n",
    "# Using Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost shap lime scikit-learn pandas numpy matplotlib seaborn --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b399c1b",
   "metadata": {},
   "source": [
    "# 2. Load the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57cf0e8",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8999a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfa55c",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617fad9",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967f620",
   "metadata": {},
   "source": [
    "# 6. Explainable AI Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840a219",
   "metadata": {},
   "source": [
    "## 6.1 SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, feature_names=iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0474ae2f",
   "metadata": {},
   "source": [
    "## 6.2 LIME (Local Interpretable Model-Agnostic Explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51aac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "idx = 5  # Pick a test instance\n",
    "exp = explainer_lime.explain_instance(X_test[idx], model.predict_proba, num_features=4)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea20ef5",
   "metadata": {},
   "source": [
    "## 6.3 Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4cfba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': iris.feature_names,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bb7b5",
   "metadata": {},
   "source": [
    "# 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to apply three different Explainable AI (XAI) techniques to interpret a machine learning model:\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations):**\n",
    "- Provided global feature importance through summary plots\n",
    "- Visualized feature interactions and dependencies\n",
    "- Showed the impact of features on individual predictions\n",
    "\n",
    "**LIME (Local Interpretable Model-Agnostic Explanations):**\n",
    "- Generated locally faithful explanations for individual predictions\n",
    "- Showed which features were most important for specific predictions\n",
    "\n",
    "**Permutation Feature Importance:**\n",
    "- Measured the decrease in model performance when features are permuted\n",
    "- Compared permutation importance with the model's built-in feature importance\n",
    "\n",
    "These techniques give us different perspectives on model interpretability:\n",
    "- SHAP provides a mathematically rigorous approach to understanding both global and local feature importance\n",
    "- LIME focuses on local explanations that are easy to understand\n",
    "- Permutation importance offers a model-agnostic way to measure feature importance\n",
    "\n",
    "**Key findings:**\n",
    "- The most important features according to all three methods were related to petal width, petal length, and sepal width\n",
    "- SHAP and permutation importance showed similar rankings for top features\n",
    "- The model performed well on the classification task with high accuracy\n",
    "\n",
    "XAI techniques help build trust in machine learning models by making their decision-making process more transparent and understandable. This is especially important in domains where model decisions have significant consequences."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
